{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from PIL import Image\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "#importing pretrained vgg19 model to calculate perceptual loss\n",
    "from torchvision.models import vgg16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "hazy_img_path ='C:\\\\Users\\\\utkar\\\\Downloads\\\\ITS\\\\hazy\\\\'\n",
    "clear_img_path = 'C:\\\\Users\\\\utkar\\\\Downloads\\\\ITS\\\\clear\\\\'\n",
    "hazy_lst = os.listdir(hazy_img_path)\n",
    "clear_lst = os.listdir(clear_img_path)\n",
    "\n",
    "hazy_list = []\n",
    "clear_list = []\n",
    "for each_hazy in hazy_lst:\n",
    "    hazy_list.append(hazy_img_path+each_hazy)\n",
    "    clear_list.append(clear_img_path+each_hazy.split('_')[0]+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size) ,\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, hazy_list, clear_list, transform,train=True):\n",
    "        self.image_paths = hazy_list\n",
    "        self.target_paths = clear_list\n",
    "        self.transforms = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        hazy_image = Image.open(self.image_paths[index])\n",
    "        clear_image = Image.open(self.target_paths[index])\n",
    "        t_hazy_image = self.transforms(hazy_image)\n",
    "        t_clear_image = self.transforms(clear_image)\n",
    "        return t_hazy_image, t_clear_image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.9\n",
    "indices = list(range(len(data_list)))\n",
    "split = int(np.floor(train_size*len(data_list)))\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "test_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "train_dataset = CustomDataset(hazy_list, clear_list,transform, train=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, num_workers=0)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=test_sampler, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = next(iter(train_dataset))\n",
    "# type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.model = nn.Sequential( \n",
    "#             nn.ConvTranspose2d(3,512,4,1,0,bias = False),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.ReLU(True),\n",
    "            \n",
    "#             nn.ConvTranspose2d(512,256,4,2,1,bias = False),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(True),\n",
    "            \n",
    "#             nn.ConvTranspose2d(256,128,4,2,1,bias = False),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(True),\n",
    "            \n",
    "#             nn.ConvTranspose2d(128,64,4,2,1,bias = False),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(True),\n",
    "            \n",
    "#             nn.ConvTranspose2d(64,3,4,2,1,bias = False),\n",
    "#             nn.Tanh()\n",
    "#         )\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         x = self.model(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3,64,4,2,1, bias = False),\n",
    "            nn.LeakyReLU(0.2,inplace = True),\n",
    "            \n",
    "            nn.Conv2d(64,128,4,2,1, bias = False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2,inplace = True),\n",
    "\n",
    "            nn.Conv2d(128,256,4,2,1, bias = False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2,inplace = True),\n",
    "\n",
    "            nn.Conv2d(256,512,4,2,1, bias = False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2,inplace = True),\n",
    "            \n",
    "            nn.Conv2d(512,1,4,1,0),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "        #return output.view(-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (main): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (12): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc = Discriminator()\n",
    "disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_img = torch.Tensor(cv2.imread('./hazy_data/1.jpg'))\n",
    "# h_img = h_img.view(1,h_img.shape[0],h_img.shape[1],h_img.shape[2])\n",
    "# h_img = h_img.permute(0,3,1,2)\n",
    "# c_img = torch.Tensor(cv2.imread('./hazy_data/2.jpg'))\n",
    "# c_img = c_img.view(1,c_img.shape[0],c_img.shape[1],c_img.shape[2])\n",
    "# c_img = c_img.permute(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perceptual_loss = 0\n",
    "# perceptual_loss_lst = []\n",
    "# percep_criterion = nn.MSELoss()\n",
    "# for clear_layer, hazy_layer in zip(clear_vgg_model,gen_vgg_model) :\n",
    "#     if isinstance(clear_layer,nn.ReLU) and isinstance(hazy_layer,nn.ReLU):\n",
    "#         #print(clear_layer.modules().item())\n",
    "#         perceptual_loss += percep_criterion(clear_layer, hazy_layer)\n",
    "\n",
    "# print(perceptual_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        vgg16_model = vgg16(pretrained = True)\n",
    "        self.vgg16 = nn.Sequential(*list(vgg16_model.features.children())[:9])\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.vgg16(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perceptual_criterion = nn.MSELoss()\n",
    "# feature_extractor = FeatureExtractor()\n",
    "# feature_extractor.eval()\n",
    "# gen_feature = feature_extractor(h_img)\n",
    "# real_feature = feature_extractor(c_img)\n",
    "\n",
    "# perceptual_loss =  perceptual_criterion(gen_feature,real_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Perceptual Loss is :-',perceptual_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Content Loss\n",
    "# content_criterion = nn.L1Loss()\n",
    "# content_loss = content_criterion(gen_img, real_img)\n",
    "\n",
    "# print('Conten Loss is :-', content_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #GAN Loss\n",
    "# gan_criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConvDownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            \n",
    "                nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.model(x)\n",
    "\n",
    "class DoubleConvUpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.LeakyReLU(0.2,inplace = True),\n",
    "                \n",
    "                nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.LeakyReLU(0.2,inplace = True))\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unet\n",
    "class GenUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "        self.dconv_down1 = DoubleConvDownBlock(3, 64)\n",
    "        self.dconv_down2 = DoubleConvDownBlock(64, 128)\n",
    "        self.dconv_down3 = DoubleConvDownBlock(128, 256)\n",
    "        self.dconv_down4 = DoubleConvDownBlock(256, 512)        \n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)        \n",
    "        \n",
    "        self.dconv_up3 = DoubleConvUpBlock(256 + 512, 256)\n",
    "        self.dconv_up2 = DoubleConvUpBlock(128 + 256, 128)\n",
    "        self.dconv_up1 = DoubleConvUpBlock(128 + 64, 64)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(64, 3,1)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        dconv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool(dconv1)\n",
    "        \n",
    "        dconv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool(dconv2)\n",
    "        \n",
    "        dconv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool(dconv3)\n",
    "        \n",
    "        dconv4 = self.dconv_down4(x)\n",
    "        \n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, dconv3], dim=1)\n",
    "        x = self.dconv_up3(x)\n",
    "        \n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, dconv2], dim=1)\n",
    "        x = self.dconv_up2(x)\n",
    "        \n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, dconv1], dim=1)\n",
    "        x = self.dconv_up1(x)\n",
    "        \n",
    "        x = self.conv_last(x)\n",
    "        x = F.tanh(x)\n",
    "        \n",
    "        return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = GenUNet()\n",
    "disc = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (main): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1))\n",
       "    (12): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureExtractor(\n",
       "  (vgg16): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training DCGans\n",
    "gan_criterion = nn.BCELoss()\n",
    "content_criterion = nn.L1Loss()\n",
    "perceptual_criterion = nn.MSELoss()\n",
    "feature_extractor = FeatureExtractor()\n",
    "feature_extractor.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerD = optim.Adam(disc.parameters(), lr = 0.0002, betas = (0.5,0.999))\n",
    "optimizerG = optim.Adam(gen.parameters(), lr = 0.0002, betas = (0.5,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 11, 15])\n",
      "torch.Size([10, 1, 11, 15])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 1. Got 74 and 75 in dimension 3 at C:\\w\\1\\s\\tmp_conda_3.6_045031\\conda\\conda-bld\\pytorch_1565412750030\\work\\aten\\src\\TH/generic/THTensor.cpp:689",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-470-b1ca28d5ca92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m#training discriminator with fake images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mfake_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhazy_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclear_images\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdisc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-465-69db04913173>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupsample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdconv3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdconv_up3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 1. Got 74 and 75 in dimension 3 at C:\\w\\1\\s\\tmp_conda_3.6_045031\\conda\\conda-bld\\pytorch_1565412750030\\work\\aten\\src\\TH/generic/THTensor.cpp:689"
     ]
    }
   ],
   "source": [
    "alpha = 1\n",
    "beta = 1\n",
    "epochs = 25\n",
    "for e in range(epochs):\n",
    "    for i, data in enumerate(train_loader):\n",
    "            \n",
    "        hazy_images, clear_images = data\n",
    "        \n",
    "        disc.zero_grad()\n",
    "        \n",
    "        #training discriminator with real images\n",
    "        target = Variable(torch.ones(clear_images.shape[0], 1,11,15))\n",
    "        print(target.shape)\n",
    "        output = disc.forward(clear_images)\n",
    "        print(output.shape)\n",
    "        errorD_real = criterion(output, target)\n",
    "        \n",
    "        \n",
    "        #training discriminator with fake images\n",
    "        fake_images = gen(hazy_images)\n",
    "        target = Variable(torch.zeros(clear_images.shape[0], 1,11,15))\n",
    "        output = disc.forward(fake_images)\n",
    "        errorD_fake = gan_criterion(output, target)\n",
    "        \n",
    "        #Total Discriminator Error\n",
    "        errorD = errorD_real + errorD_fake\n",
    "        errorD.backward()\n",
    "        optimizerD.step()\n",
    "        \n",
    "        \n",
    "        #update the weights of Generator, now the target variable will be 1 as we want the generator to generate real images\n",
    "        gen.zero_grad()\n",
    "        \n",
    "        target = Variable(torch.ones(real_images.size()[0], 1,11,15))\n",
    "        output = disc(fake_images)\n",
    "        gan_loss = gan_criterion(output, target)\n",
    "        \n",
    "        #content loss\n",
    "        content_loss = content_criterion(fake_images, clear_images)\n",
    "        \n",
    "        #perceptual loss\n",
    "        gen_feature = feature_extractor(fake_images)\n",
    "        real_feature = feature_extractor(clear_images)\n",
    "        perceptual_loss =  perceptual_criterion(gen_feature,real_feature).detach()\n",
    "           \n",
    "        #total Generator loss\n",
    "        errorG = ganloss + alpha*content_loss + beta*perceptual_loss       \n",
    "        errorG.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        \n",
    "        print('{:4f} Generator Loss :- {:6f}, Discriminator Loss :- {:6f}'.format(e, errorG.items(), errorD.items()))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list[588]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
